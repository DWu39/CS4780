1c)
To run 1c.py you just have to make sure 1c.py, the digits folder, and the svm_light folder are in the same directory. No external libraries were used except svm_light and sklearn for loading the svmlight file.

We picked the 10 models used on the testing set based on whichever C gave the highest accuracy on the validation set. If there were ties, we chose the lower C for a lower chance of over fitting. Although we did not do multi-k-fold cross validation, this is the best guess we have for choosing the best C.

All accuracies during validation for each digit while varying C:
   .0001 .0005 .001  .005   .01   .05   .1
10 100.0 100.0 100.0 100.0 100.0 100.0 100.0
1  97.71 97.71 97.71 97.96 97.96 97.71 97.46
2  99.22 99.22 99.22 99.22 99.22 98.96 98.96
3  99.48 99.22 98.96 98.96 98.96 98.96 98.96
4  99.2  99.2  99.2  99.46 99.73 99.46 98.93
5  99.74 100.0 100.0 100.0 100.0 99.48 100.0
6  99.74 100.0 100.0 100.0 100.0 99.48 99.22
7  98.93 99.2  99.2  99.2  98.93 99.2  99.2
8  94.75 95.28 95.8  95.54 95.54 95.54 96.06
9  96.83 97.88 98.41 98.15 97.88 98.15 98.15

The best C value for each digit was:
10:0.0001, 1:0.005, 2:0.0001, 3:0.0001, 4:0.01, 5:0.0005, 6:0.0005, 7:0.0005, 8:0.1, 9:0.001

The accuracy on the overall test set was: 93.7117417919%

Command for training n=10 classifiers.
svm_learn -c 'c' digitsn.train modeln. 'c' was all of the C values. And n was from 0-9 for the digits. -t did not need to be set since the default is a linear kernel.

1d)
Running 1d is the same as running 1c.

When using non-linear polynomial kernels, varying the values of C does not seem to change the accuracy of the classifier. We picked d = 4 since looking at the accuracies from the predictions on the validation sets, it was clear that 4 was the only degree giving all 10 classifiers a max accuracy. Since C has no effect on the classifiers, I decided to pick C = 0.0001 for each one in that case to reduce the chance of over fitting.

The accuracy on the overall test set was: 98.0523094046%

Command for training n=10 classifiers.
svm_learn -t 1 -d 'd' -c 'c' digitsn.train modeln. 'd' was 2,3,4 and 5. 'c' was all of the C values. And n was from 0-9 for the digits.

3a)
To run 3a.py and 3b.py, just make sure the arxiv folder is in the same directory.
We obtained an accuracy of: 96.0414935%
false positives: 394
false negatives: 892

The NB classifier does marginally worse than a linear SVM.

3b)
We obtained an accuracy of: 96.010712%
false positives: 467
false negatives: 829

Our accuracy decreased slightly. We care much more about false negatives than false positives. The cost of missing an astrophysics paper is much higher than reading extra papers. In this case, 10x much more costly.

That means we are saying we are willing to misclassify more articles as astrophysics in order to have a higher recall to get more of the astrophysics articles. The higher recall comes at a cost of lower precision so our accuracy goes down.

Also, if we look at the decision rule it is 10*Pr(Y=+1|X=x) >= Pr(Y=-1|X=x). This means that in order to classify an article as negative, the posterior probability for the negative class must 10x that of the positive class. Of course this would lead to a lower accuracy as the negative examples with lower probabilities of being negative (<10%) will now be classified as positive.